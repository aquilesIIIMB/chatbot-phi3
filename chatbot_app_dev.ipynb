{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    from optimum.exporters.onnx.model_configs import NormalizedConfig\n",
    "\n",
    "    class Phi3NormalizedConfig(NormalizedConfig):\n",
    "        \"\"\"\n",
    "        Minimal normalized config for a Phi3-based causal language model.\n",
    "        Feel free to expand with other fields as needed by your model.\n",
    "        \"\"\"\n",
    "        def __init__(self, config):\n",
    "            super().__init__(config)\n",
    "            # Assign typical fields for a causal LM:\n",
    "            self._num_layers = getattr(config, \"num_hidden_layers\", None)\n",
    "            self._hidden_size = getattr(config, \"hidden_size\", None)\n",
    "            self._num_attention_heads = getattr(config, \"num_attention_heads\", None)\n",
    "            # Optionally store the model type\n",
    "            self._model_type = getattr(config, \"model_type\", \"phi3\")\n",
    "\n",
    "        @property\n",
    "        def num_layers(self) -> int:\n",
    "            return self._num_layers\n",
    "\n",
    "        @property\n",
    "        def hidden_size(self) -> int:\n",
    "            return self._hidden_size\n",
    "\n",
    "        @property\n",
    "        def num_attention_heads(self) -> int:\n",
    "            return self._num_attention_heads\n",
    "\n",
    "        @property\n",
    "        def model_type(self) -> str:\n",
    "            return self._model_type\n",
    "        \n",
    "    from optimum.exporters.onnx import OnnxConfig\n",
    "\n",
    "    class Phi3CustomOnnxConfig(OnnxConfig):\n",
    "        \"\"\"\n",
    "        Minimal OnnxConfig that declares the input/output specifications\n",
    "        for a Phi3-based causal LM. \n",
    "        \"\"\"\n",
    "        NORMALIZED_CONFIG_CLASS = Phi3NormalizedConfig\n",
    "\n",
    "        @property\n",
    "        def inputs(self):\n",
    "            # For causal LM inference, we typically need `input_ids` and `attention_mask`.\n",
    "            # We map dimensions to ONNX-friendly names like \"batch_size\" and \"sequence_length\".\n",
    "            return {\n",
    "                \"input_ids\": {\n",
    "                    0: \"batch_size\",\n",
    "                    1: \"sequence_length\"\n",
    "                },\n",
    "                \"attention_mask\": {\n",
    "                    0: \"batch_size\",\n",
    "                    1: \"sequence_length\"\n",
    "                },\n",
    "            }\n",
    "\n",
    "        @property\n",
    "        def outputs(self):\n",
    "            # The usual main output is logits of shape (batch_size, sequence_length, vocab_size).\n",
    "            return {\n",
    "                \"logits\": {\n",
    "                    0: \"batch_size\",\n",
    "                    1: \"sequence_length\"\n",
    "                },\n",
    "            }\n",
    "\n",
    "        @property\n",
    "        def default_onnx_opset_version(self) -> int:\n",
    "            return 15  # Usually safe to go with opset 13 or 15\n",
    "\n",
    "        def generate_dummy_inputs(self, framework: str = \"pt\", **kwargs):\n",
    "            \"\"\"\n",
    "            Provide minimal dummy tensors to trace the model.\n",
    "            Adjust shapes as needed for your typical input size.\n",
    "            \"\"\"\n",
    "            import torch\n",
    "\n",
    "            batch_size = 1\n",
    "            seq_length = 4\n",
    "\n",
    "            dummy_input_ids = torch.randint(\n",
    "                low=0,\n",
    "                high=1000,\n",
    "                size=(batch_size, seq_length),\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            dummy_attention_mask = torch.ones(\n",
    "                (batch_size, seq_length),\n",
    "                dtype=torch.long\n",
    "            )\n",
    "            return {\n",
    "                \"input_ids\": dummy_input_ids,\n",
    "                \"attention_mask\": dummy_attention_mask\n",
    "            }\n",
    "    \n",
    "    \n",
    "    from optimum.exporters.onnx import export\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "    from pathlib import Path\n",
    "\n",
    "    model_id = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "    onnx_dir = Path(\"./phi3_mini_onnx\")\n",
    "    onnx_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1. Load the model and config with `trust_remote_code=True`\n",
    "    config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "    # 2. Create an instance of your custom OnnxConfig\n",
    "    onnx_config = Phi3CustomOnnxConfig(config)\n",
    "\n",
    "    # 3. Export the model\n",
    "    export(\n",
    "        model=model,\n",
    "        config=onnx_config,\n",
    "        output=onnx_dir / \"model.onnx\",\n",
    "        opset=15  # or 13, matching your custom config\n",
    "    )\n",
    "\n",
    "    # 4. Save tokenizer & config\n",
    "    tokenizer.save_pretrained(onnx_dir)\n",
    "    config.save_pretrained(onnx_dir)\n",
    "    print(\"Done exporting Phi-3 Mini to ONNX.\")\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    from optimum.onnxruntime import ORTQuantizer\n",
    "    from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "    from pathlib import Path\n",
    "\n",
    "    onnx_dir = Path(\"./phi3_mini_onnx\")\n",
    "    quantized_path = Path(\"./phi3_mini_quantized\")\n",
    "    quantized_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    quantizer = ORTQuantizer.from_pretrained(onnx_dir)\n",
    "\n",
    "    quant_config = AutoQuantizationConfig.arm64(\n",
    "        is_static=False,    # dynamic quantization\n",
    "        per_channel=True    # might produce smaller result, but more memory usage\n",
    "    )\n",
    "\n",
    "    # Note: The key fix is `use_external_data_format=True`\n",
    "    quantizer.quantize(\n",
    "        quantization_config=quant_config,\n",
    "        save_dir=quantized_path,\n",
    "        use_external_data_format=True\n",
    "    )\n",
    "\n",
    "    print(\"Quantized model saved to:\", quantized_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "\n",
    "# Load Phi-3 mini model and tokenizer\n",
    "model_id = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # Use half precision for efficiency\n",
    "    device_map=\"auto\"  # Automatically choose best device (CPU/GPU)\n",
    ")\n",
    "# Benchmark quantized model performance\n",
    "model_quantized = ORTModelForCausalLM.from_pretrained(\"./phi3_mini_quantized\", use_io_binding=False, use_cache=False)\n",
    "\n",
    "# Test basic inference\n",
    "def generate_response(model, prompt, max_length=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.replace(prompt, \"\").strip()\n",
    "\n",
    "# Test with different prompts\n",
    "test_prompts = [\n",
    "    \"Hello, who are you?\",\n",
    "    \"Can you help me find a good restaurant?\",\n",
    "    \"What's the capital of France?\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generate_response(model, prompt)\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "    \n",
    "for prompt in test_prompts:\n",
    "    response = generate_response(model_quantized, prompt)\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")\n",
    "    \n",
    "\n",
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def benchmark_inference(model, prompt, runs=5):\n",
    "    memory_usage = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for _ in range(runs):\n",
    "        # Record memory before\n",
    "        mem_before = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        # Time inference\n",
    "        start_time = time.time()\n",
    "        _ = generate_response(model, prompt)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Record memory after\n",
    "        mem_after = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "        memory_usage.append(mem_after - mem_before)\n",
    "    \n",
    "    return {\n",
    "        \"avg_inference_time\": sum(inference_times) / runs,\n",
    "        \"avg_memory_usage\": sum(memory_usage) / runs\n",
    "    }\n",
    "\n",
    "# Run benchmarks\n",
    "results = benchmark_inference(model, \"Tell me about yourself and what you can do.\")\n",
    "print(f\"Average inference time: {results['avg_inference_time']:.2f} seconds\")\n",
    "print(f\"Average memory usage: {results['avg_memory_usage']:.2f} MB\")\n",
    "\n",
    "# Run benchmarks\n",
    "results = benchmark_inference(model_quantized, \"Tell me about yourself and what you can do.\")\n",
    "print(f\"Average inference time: {results['avg_inference_time']:.2f} seconds\")\n",
    "print(f\"Average memory usage: {results['avg_memory_usage']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing an On-Device AI Chatbot Powered by Phi-3 Mini: A Complete Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building an on-device AI chatbot with Phi-3 mini requires careful planning from model optimization to deployment. I'll walk you through the entire process of creating a privacy-focused, personalized AI companion that can run efficiently on mobile devices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Model Testing and Exploration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set Up Your Development Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create and activate a virtual environment\n",
    "python -m venv phi3_env\n",
    "source phi3_env/bin/activate  # On Windows: phi3_env\\Scripts\\activate\n",
    "\n",
    "# Install required packages\n",
    "pip install torch transformers datasets evaluate accelerate jupyter matplotlib numpy pandas\n",
    "pip install tensorboard optimum onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Alternatively, you can run the following commands in your terminal to install the required packages:\n",
    "# Verify Nvidia GPU driver installation and CUDA version\n",
    "nvidia-smi\n",
    "nvcc --version\n",
    "ldd --version\n",
    "strings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 | grep GLIBCXX\n",
    "strings /usr/local/libstdcxx/lib64/libstdc++.so.6 | grep GLIBCXX\n",
    "\n",
    "\n",
    "import torch\n",
    "print(torch.__version__, torch.version.cuda, torch.cuda.is_available())\n",
    "\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "pip install torch==2.0.1+cu118 \\\n",
    "            torchvision==0.15.2+cu118 \\\n",
    "            torchaudio==2.0.1+cu118 \\\n",
    "            --extra-index-url https://download.pytorch.org/whl/cu118\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initial Model Testing in Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new notebook to explore Phi-3 mini capabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load Phi-3 mini model and tokenizer\n",
    "model_id = \"microsoft/phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,  # Use half precision for efficiency\n",
    "    device_map=\"auto\"  # Automatically choose best device (CPU/GPU)\n",
    ")\n",
    "\n",
    "# Test basic inference\n",
    "def generate_response(prompt, max_length=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate response\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    # Decode the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response.replace(prompt, \"\").strip()\n",
    "\n",
    "# Test with different prompts\n",
    "test_prompts = [\n",
    "    \"Hello, who are you?\",\n",
    "    \"Can you help me find a good restaurant?\",\n",
    "    \"What's the capital of France?\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"Prompt: {prompt}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Benchmark Model Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def benchmark_inference(prompt, runs=5):\n",
    "    memory_usage = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for _ in range(runs):\n",
    "        # Record memory before\n",
    "        mem_before = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        # Time inference\n",
    "        start_time = time.time()\n",
    "        _ = generate_response(prompt)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Record memory after\n",
    "        mem_after = psutil.Process().memory_info().rss / 1024 / 1024  # MB\n",
    "        \n",
    "        inference_times.append(end_time - start_time)\n",
    "        memory_usage.append(mem_after - mem_before)\n",
    "    \n",
    "    return {\n",
    "        \"avg_inference_time\": sum(inference_times) / runs,\n",
    "        \"avg_memory_usage\": sum(memory_usage) / runs\n",
    "    }\n",
    "\n",
    "# Run benchmarks\n",
    "results = benchmark_inference(\"Tell me about yourself and what you can do.\")\n",
    "print(f\"Average inference time: {results['avg_inference_time']:.2f} seconds\")\n",
    "print(f\"Average memory usage: {results['avg_memory_usage']:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Model Optimization for Mobile Deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Quantize the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Export model to ONNX format\n",
    "from optimum.onnxruntime import ORTModelForCausalLM\n",
    "\n",
    "# First convert to ONNX\n",
    "onnx_model_path = \"./phi3_mini_onnx\"\n",
    "ort_model = ORTModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    export=True,\n",
    "    provider=\"CPUExecutionProvider\"\n",
    ")\n",
    "ort_model.save_pretrained(onnx_model_path)\n",
    "\n",
    "# Then quantize\n",
    "quantizer = ORTQuantizer.from_pretrained(onnx_model_path)\n",
    "qconfig = AutoQuantizationConfig.arm64(is_static=True, per_channel=False)\n",
    "quantizer.quantize(quantization_config=qconfig, save_dir=\"./phi3_mini_quantized\")\n",
    "\n",
    "# Benchmark quantized model performance\n",
    "model_quantized = ORTModelForCausalLM.from_pretrained(\"./phi3_mini_quantized\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Test inference speed and compare to original model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Optimize Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pruning example - removing less important weights\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers.pruning_utils import prune_linear_layer\n",
    "import torch\n",
    "\n",
    "def prune_model(model, pruning_threshold=0.1):\n",
    "    \"\"\"Prune model weights below a certain threshold\"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Get weight magnitudes\n",
    "            weight_magnitudes = torch.abs(module.weight)\n",
    "            \n",
    "            # Create mask for weights below threshold\n",
    "            mask = weight_magnitudes < pruning_threshold\n",
    "            \n",
    "            # Set small weights to zero\n",
    "            module.weight.data[mask] = 0\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Apply pruning\n",
    "pruned_model = prune_model(model, pruning_threshold=0.05)\n",
    "\n",
    "# Save pruned model\n",
    "pruned_model.save_pretrained(\"./phi3_mini_pruned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Export for Mobile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For iOS (Core ML)\n",
    "import coremltools as ct\n",
    "\n",
    "# Convert to Core ML format\n",
    "mlmodel = ct.convert(\n",
    "    \"phi3_mini_quantized\",\n",
    "    source=\"onnx\",\n",
    "    minimum_deployment_target=ct.target.iOS15\n",
    ")\n",
    "mlmodel.save(\"PhiMiniModel.mlpackage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Backend Development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create Project Structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p phi3_chatbot/{api,models,utils,config,tests}\n",
    "cd phi3_chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Implement Basic API with FastAPI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `main.py` file in the api directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException, Depends\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "import uuid\n",
    "import datetime\n",
    "\n",
    "app = FastAPI(title=\"Phi-3 Mini Chatbot API\")\n",
    "\n",
    "# --- Data Models ---\n",
    "class Message(BaseModel):\n",
    "    role: str  # 'user' or 'assistant'\n",
    "    content: str\n",
    "    timestamp: datetime.datetime = datetime.datetime.now()\n",
    "\n",
    "class Conversation(BaseModel):\n",
    "    id: str = str(uuid.uuid4())\n",
    "    messages: List[Message] = []\n",
    "    created_at: datetime.datetime = datetime.datetime.now()\n",
    "    updated_at: datetime.datetime = datetime.datetime.now()\n",
    "\n",
    "class ChatRequest(BaseModel):\n",
    "    conversation_id: Optional[str] = None\n",
    "    message: str\n",
    "    system_prompt: Optional[str] = None\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    conversation_id: str\n",
    "    response: str\n",
    "    \n",
    "# In-memory store for conversations (replace with proper DB in production)\n",
    "conversations = {}\n",
    "\n",
    "# --- Model Initialization ---\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"../phi3_mini_quantized\"  # Path to your quantized model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# --- API Endpoints ---\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    # Get or create conversation\n",
    "    if request.conversation_id and request.conversation_id in conversations:\n",
    "        conversation = conversations[request.conversation_id]\n",
    "    else:\n",
    "        conversation = Conversation()\n",
    "        conversations[conversation.id] = conversation\n",
    "    \n",
    "    # Add user message\n",
    "    conversation.messages.append(Message(role=\"user\", content=request.message))\n",
    "    \n",
    "    # Prepare prompt with conversation history\n",
    "    system_prompt = request.system_prompt or \"You are a helpful AI assistant running on-device.\"\n",
    "    prompt = f\"{system_prompt}\\n\\n\"\n",
    "    \n",
    "    # Add conversation history\n",
    "    for msg in conversation.messages[-5:]:  # Only use last 5 messages for context\n",
    "        prompt += f\"{msg.role.capitalize()}: {msg.content}\\n\"\n",
    "    \n",
    "    prompt += \"Assistant: \"\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    response_text = response_text.replace(prompt, \"\").strip()\n",
    "    \n",
    "    # Add assistant message to conversation\n",
    "    conversation.messages.append(Message(role=\"assistant\", content=response_text))\n",
    "    conversation.updated_at = datetime.datetime.now()\n",
    "    \n",
    "    return ChatResponse(\n",
    "        conversation_id=conversation.id,\n",
    "        response=response_text\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Implement Conversation Memory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `memory.py` file in the utils directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "import datetime\n",
    "\n",
    "class ConversationMemory:\n",
    "    def __init__(self, max_history=10):\n",
    "        self.max_history = max_history\n",
    "        self.conversations = {}\n",
    "    \n",
    "    def add_message(self, conversation_id, role, content):\n",
    "        # Create conversation if it doesn't exist\n",
    "        if conversation_id not in self.conversations:\n",
    "            self.conversations[conversation_id] = {\n",
    "                \"messages\": [],\n",
    "                \"created_at\": datetime.datetime.now(),\n",
    "                \"metadata\": {}\n",
    "            }\n",
    "        \n",
    "        # Add message\n",
    "        self.conversations[conversation_id][\"messages\"].append({\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.datetime.now()\n",
    "        })\n",
    "        \n",
    "        # Trim history if needed\n",
    "        if len(self.conversations[conversation_id][\"messages\"]) > self.max_history:\n",
    "            self.conversations[conversation_id][\"messages\"] = self.conversations[conversation_id][\"messages\"][-self.max_history:]\n",
    "    \n",
    "    def get_conversation(self, conversation_id):\n",
    "        return self.conversations.get(conversation_id, None)\n",
    "    \n",
    "    def get_messages(self, conversation_id, limit=None):\n",
    "        if conversation_id not in self.conversations:\n",
    "            return []\n",
    "        \n",
    "        messages = self.conversations[conversation_id][\"messages\"]\n",
    "        if limit:\n",
    "            return messages[-limit:]\n",
    "        return messages\n",
    "    \n",
    "    def store_metadata(self, conversation_id, key, value):\n",
    "        if conversation_id not in self.conversations:\n",
    "            return False\n",
    "        \n",
    "        self.conversations[conversation_id][\"metadata\"][key] = value\n",
    "        return True\n",
    "    \n",
    "    def get_metadata(self, conversation_id, key=None):\n",
    "        if conversation_id not in self.conversations:\n",
    "            return None\n",
    "        \n",
    "        if key:\n",
    "            return self.conversations[conversation_id][\"metadata\"].get(key, None)\n",
    "        return self.conversations[conversation_id][\"metadata\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4: Cloud Infrastructure Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Set Up Cloud Resources on GCP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install Google Cloud SDK\n",
    "# https://cloud.google.com/sdk/docs/install\n",
    "\n",
    "# Initialize GCP\n",
    "gcloud init\n",
    "\n",
    "# Create a new project\n",
    "gcloud projects create phi3-chatbot-app\n",
    "\n",
    "# Set the project as active\n",
    "gcloud config set project phi3-chatbot-app\n",
    "\n",
    "# Enable required APIs\n",
    "gcloud services enable artifactregistry.googleapis.com\n",
    "gcloud services enable run.googleapis.com\n",
    "gcloud services enable firestore.googleapis.com\n",
    "\n",
    "# Create a Docker repository in Artifact Registry\n",
    "gcloud artifacts repositories create phi3-chatbot-repo \\\n",
    "    --repository-format=docker \\\n",
    "    --location=us-central1 \\\n",
    "    --description=\"Docker repository for Phi-3 Chatbot\"\n",
    "\n",
    "# Create a Firestore database\n",
    "gcloud firestore databases create --region=us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Configure Cloud Storage for Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create Cloud Storage bucket for model files\n",
    "gcloud storage buckets create gs://phi3-chatbot-models \\\n",
    "    --location=us-central1 \\\n",
    "    --uniform-bucket-level-access\n",
    "\n",
    "# Upload model files to bucket\n",
    "gcloud storage cp ./phi3_mini_quantized/* gs://phi3-chatbot-models/phi3_mini_quantized/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Set Up Containerization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `Dockerfile`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "FROM python:3.10-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    build-essential \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements and install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy model files and application code\n",
    "COPY . .\n",
    "\n",
    "# Download model from GCS at build time\n",
    "RUN mkdir -p phi3_mini_quantized\n",
    "RUN pip install google-cloud-storage\n",
    "RUN python -c \"from google.cloud import storage; \\\n",
    "    client = storage.Client(); \\\n",
    "    bucket = client.bucket('phi3-chatbot-models'); \\\n",
    "    blobs = bucket.list_blobs(prefix='phi3_mini_quantized/'); \\\n",
    "    for blob in blobs: \\\n",
    "        if blob.name.endswith('/'): continue; \\\n",
    "        target = blob.name.replace('phi3_mini_quantized/', 'phi3_mini_quantized/'); \\\n",
    "        blob.download_to_filename(target);\"\n",
    "\n",
    "# Set the environment variable\n",
    "ENV MODEL_PATH=/app/phi3_mini_quantized\n",
    "\n",
    "# Expose port 8000\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run the application\n",
    "CMD [\"uvicorn\", \"api.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `requirements.txt` file:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "fastapi==0.103.1\n",
    "uvicorn==0.23.2\n",
    "transformers==4.35.0\n",
    "optimum==1.14.0\n",
    "onnxruntime==1.16.0\n",
    "pydantic==2.4.2\n",
    "google-cloud-storage==2.12.0\n",
    "google-cloud-firestore==2.13.1\n",
    "python-multipart==0.0.6\n",
    "torch==2.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5: Mobile App Development\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Create a Simple React Native Frontend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install React Native CLI\n",
    "npm install -g react-native-cli\n",
    "\n",
    "# Create a new React Native project\n",
    "npx react-native init Phi3ChatbotApp\n",
    "\n",
    "# Navigate to the project directory\n",
    "cd Phi3ChatbotApp\n",
    "\n",
    "# Install required dependencies\n",
    "npm install @react-navigation/native @react-navigation/stack\n",
    "npm install react-native-gesture-handler react-native-safe-area-context\n",
    "npm install axios react-native-gifted-chat react-native-vector-icons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Implement Chat Interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `ChatScreen.js` file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "import React, { useState, useCallback, useEffect } from 'react';\n",
    "import { GiftedChat } from 'react-native-gifted-chat';\n",
    "import { ActivityIndicator, View, Text, StyleSheet } from 'react-native';\n",
    "import axios from 'axios';\n",
    "\n",
    "const API_URL = 'https://your-api-url.com'; // Replace with your actual API URL\n",
    "\n",
    "const ChatScreen = () => {\n",
    "  const [messages, setMessages] = useState([]);\n",
    "  const [conversationId, setConversationId] = useState(null);\n",
    "  const [loading, setLoading] = useState(false);\n",
    "\n",
    "  useEffect(() => {\n",
    "    // Initialize with a welcome message\n",
    "    setMessages([\n",
    "      {\n",
    "        _id: 1,\n",
    "        text: 'Hello! I\\'m your Phi-3 powered AI assistant. How can I help you today?',\n",
    "        createdAt: new Date(),\n",
    "        user: {\n",
    "          _id: 2,\n",
    "          name: 'AI Assistant',\n",
    "          avatar: 'https://placeimg.com/140/140/tech',\n",
    "        },\n",
    "      },\n",
    "    ]);\n",
    "  }, []);\n",
    "\n",
    "  const onSend = useCallback((newMessages = []) => {\n",
    "    setMessages(previousMessages => \n",
    "      GiftedChat.append(previousMessages, newMessages)\n",
    "    );\n",
    "    \n",
    "    // Send message to API\n",
    "    const userMessage = newMessages[0].text;\n",
    "    setLoading(true);\n",
    "    \n",
    "    axios.post(`${API_URL}/chat`, {\n",
    "      conversation_id: conversationId,\n",
    "      message: userMessage,\n",
    "    })\n",
    "    .then(response => {\n",
    "      // Save conversation ID for future messages\n",
    "      if (!conversationId) {\n",
    "        setConversationId(response.data.conversation_id);\n",
    "      }\n",
    "      \n",
    "      // Add AI response to chat\n",
    "      const aiMessage = {\n",
    "        _id: Math.round(Math.random() * 1000000),\n",
    "        text: response.data.response,\n",
    "        createdAt: new Date(),\n",
    "        user: {\n",
    "          _id: 2,\n",
    "          name: 'AI Assistant',\n",
    "          avatar: 'https://placeimg.com/140/140/tech',\n",
    "        },\n",
    "      };\n",
    "      \n",
    "      setMessages(previousMessages => \n",
    "        GiftedChat.append(previousMessages, [aiMessage])\n",
    "      );\n",
    "    })\n",
    "    .catch(error => {\n",
    "      console.error('Error sending message:', error);\n",
    "      \n",
    "      // Show error message\n",
    "      const errorMessage = {\n",
    "        _id: Math.round(Math.random() * 1000000),\n",
    "        text: 'Sorry, I encountered an error. Please try again.',\n",
    "        createdAt: new Date(),\n",
    "        user: {\n",
    "          _id: 2,\n",
    "          name: 'AI Assistant',\n",
    "          avatar: 'https://placeimg.com/140/140/tech',\n",
    "        },\n",
    "      };\n",
    "      \n",
    "      setMessages(previousMessages => \n",
    "        GiftedChat.append(previousMessages, [errorMessage])\n",
    "      );\n",
    "    })\n",
    "    .finally(() => {\n",
    "      setLoading(false);\n",
    "    });\n",
    "  }, [conversationId]);\n",
    "\n",
    "  return (\n",
    "    <View style={styles.container}>\n",
    "      {loading && (\n",
    "        <View style={styles.loadingContainer}>\n",
    "          <ActivityIndicator size=\"large\" color=\"#0000ff\" />\n",
    "          <Text style={styles.loadingText}>Thinking...</Text>\n",
    "        </View>\n",
    "      )}\n",
    "      <GiftedChat\n",
    "        messages={messages}\n",
    "        onSend={messages => onSend(messages)}\n",
    "        user={{\n",
    "          _id: 1,\n",
    "        }}\n",
    "        renderAvatar={null}\n",
    "        alwaysShowSend\n",
    "        scrollToBottom\n",
    "      />\n",
    "    </View>\n",
    "  );\n",
    "};\n",
    "\n",
    "const styles = StyleSheet.create({\n",
    "  container: {\n",
    "    flex: 1,\n",
    "    backgroundColor: '#f5f5f5',\n",
    "  },\n",
    "  loadingContainer: {\n",
    "    position: 'absolute',\n",
    "    top: 10,\n",
    "    left: 0,\n",
    "    right: 0,\n",
    "    zIndex: 1,\n",
    "    alignItems: 'center',\n",
    "    justifyContent: 'center',\n",
    "    flexDirection: 'row',\n",
    "    backgroundColor: 'rgba(255, 255, 255, 0.8)',\n",
    "    padding: 10,\n",
    "    borderRadius: 20,\n",
    "    marginHorizontal: 20,\n",
    "  },\n",
    "  loadingText: {\n",
    "    marginLeft: 10,\n",
    "    fontSize: 16,\n",
    "  },\n",
    "});\n",
    "\n",
    "export default ChatScreen;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 15: Implement On-Device Model Integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For iOS, create a Swift bridge to use the Core ML model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "// PhiModelManager.swift\n",
    "\n",
    "import Foundation\n",
    "import CoreML\n",
    "\n",
    "class PhiModelManager {\n",
    "    static let shared = PhiModelManager()\n",
    "    \n",
    "    private var model: MLModel?\n",
    "    \n",
    "    private init() {\n",
    "        loadModel()\n",
    "    }\n",
    "    \n",
    "    private func loadModel() {\n",
    "        do {\n",
    "            // Load the Core ML model\n",
    "            let modelURL = Bundle.main.url(forResource: \"PhiMiniModel\", withExtension: \"mlmodelc\")!\n",
    "            model = try MLModel(contentsOf: modelURL)\n",
    "            print(\"Successfully loaded Phi-3 model\")\n",
    "        } catch {\n",
    "            print(\"Error loading model: \\(error)\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    func generateResponse(for prompt: String, completion: @escaping (String?, Error?) -> Void) {\n",
    "        guard let model = model else {\n",
    "            completion(nil, NSError(domain: \"PhiModelError\", code: 1, userInfo: [NSLocalizedDescriptionKey: \"Model not loaded\"]))\n",
    "            return\n",
    "        }\n",
    "        \n",
    "        // Prepare input\n",
    "        guard let input = try? MLDictionaryFeatureProvider(dictionary: [\"prompt\": prompt as NSString]) else {\n",
    "            completion(nil, NSError(domain: \"PhiModelError\", code: 2, userInfo: [NSLocalizedDescriptionKey: \"Failed to create input\"]))\n",
    "            return\n",
    "        }\n",
    "        \n",
    "        // Perform prediction\n",
    "        DispatchQueue.global(qos: .userInitiated).async {\n",
    "            do {\n",
    "                let prediction = try model.prediction(from: input)\n",
    "                \n",
    "                // Extract text output\n",
    "                if let textOutput = prediction.featureValue(for: \"text\")?.stringValue {\n",
    "                    DispatchQueue.main.async {\n",
    "                        completion(textOutput, nil)\n",
    "                    }\n",
    "                } else {\n",
    "                    DispatchQueue.main.async {\n",
    "                        completion(nil, NSError(domain: \"PhiModelError\", code: 3, userInfo: [NSLocalizedDescriptionKey: \"No output generated\"]))\n",
    "                    }\n",
    "                }\n",
    "            } catch {\n",
    "                DispatchQueue.main.async {\n",
    "                    completion(nil, error)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 6: Deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Deploy Backend to Cloud Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Build and push Docker image to Artifact Registry\n",
    "gcloud builds submit --tag us-central1-docker.pkg.dev/phi3-chatbot-app/phi3-chatbot-repo/phi3-api:v1\n",
    "\n",
    "# Deploy to Cloud Run\n",
    "gcloud run deploy phi3-api \\\n",
    "    --image us-central1-docker.pkg.dev/phi3-chatbot-app/phi3-chatbot-repo/phi3-api:v1 \\\n",
    "    --platform managed \\\n",
    "    --region us-central1 \\\n",
    "    --memory 2Gi \\\n",
    "    --cpu 2 \\\n",
    "    --allow-unauthenticated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17: Set Up CI/CD with GitHub Actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `.github/workflows/deploy.yml` file:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "name: Build and Deploy\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  build-and-deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    \n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Set up Cloud SDK\n",
    "      uses: google-github-actions/setup-gcloud@v1\n",
    "      with:\n",
    "        project_id: phi3-chatbot-app\n",
    "        service_account_key: ${{ secrets.GCP_SA_KEY }}\n",
    "    \n",
    "    - name: Authenticate Docker to Google Artifact Registry\n",
    "      run: |\n",
    "        gcloud auth configure-docker us-central1-docker.pkg.dev\n",
    "    \n",
    "    - name: Build and push Docker image\n",
    "      run: |\n",
    "        docker build -t us-central1-docker.pkg.dev/phi3-chatbot-app/phi3-chatbot-repo/phi3-api:${{ github.sha }} .\n",
    "        docker push us-central1-docker.pkg.dev/phi3-chatbot-app/phi3-chatbot-repo/phi3-api:${{ github.sha }}\n",
    "    \n",
    "    - name: Deploy to Cloud Run\n",
    "      run: |\n",
    "        gcloud run deploy phi3-api \\\n",
    "          --image us-central1-docker.pkg.dev/phi3-chatbot-app/phi3-chatbot-repo/phi3-api:${{ github.sha }} \\\n",
    "          --platform managed \\\n",
    "          --region us-central1 \\\n",
    "          --memory 2Gi \\\n",
    "          --cpu 2 \\\n",
    "          --allow-unauthenticated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 18: Prepare Mobile App for Release\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For iOS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Configure app for production\n",
    "cd ios\n",
    "pod install\n",
    "\n",
    "# Build the app for production\n",
    "xcodebuild -workspace Phi3ChatbotApp.xcworkspace -scheme Phi3ChatbotApp -configuration Release -destination 'generic/platform=iOS' -archivePath Phi3ChatbotApp.xcarchive archive\n",
    "\n",
    "# Create an IPA file\n",
    "xcodebuild -exportArchive -archivePath Phi3ChatbotApp.xcarchive -exportOptionsPlist ExportOptions.plist -exportPath ./build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 7: Testing and Quality Assurance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 19: Implement Automated Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `test_api.py` file in the tests directory:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from fastapi.testclient import TestClient\n",
    "from api.main import app\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def test_chat_endpoint_new_conversation():\n",
    "    response = client.post(\n",
    "        \"/chat\",\n",
    "        json={\"message\": \"Hello, how are you?\"}\n",
    "    )\n",
    "    assert response.status_code == 200\n",
    "    assert \"conversation_id\" in response.json()\n",
    "    assert \"response\" in response.json()\n",
    "    assert len(response.json()[\"response\"]) > 0\n",
    "\n",
    "def test_chat_endpoint_existing_conversation():\n",
    "    # Create a conversation first\n",
    "    response1 = client.post(\n",
    "        \"/chat\",\n",
    "        json={\"message\": \"What is your name?\"}\n",
    "    )\n",
    "    conversation_id = response1.json()[\"conversation_id\"]\n",
    "    \n",
    "    # Continue the conversation\n",
    "    response2 = client.post(\n",
    "        \"/chat\",\n",
    "        json={\n",
    "            \"conversation_id\": conversation_id,\n",
    "            \"message\": \"What can you do?\"\n",
    "        }\n",
    "    )\n",
    "    assert response2.status_code == 200\n",
    "    assert response2.json()[\"conversation_id\"] == conversation_id\n",
    "    assert \"response\" in response2.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 20: Manual Testing Checklist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a `testing_checklist.md` file:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Manual Testing Checklist\n",
    "\n",
    "## Backend API Testing\n",
    "- [ ] Verify chat endpoint responses\n",
    "- [ ] Test conversation continuity\n",
    "- [ ] Measure response times\n",
    "- [ ] Test error handling\n",
    "- [ ] Verify memory functionality\n",
    "\n",
    "## On-Device Model Testing\n",
    "- [ ] Test inference speed\n",
    "- [ ] Measure memory usage\n",
    "- [ ] Verify response quality\n",
    "- [ ] Test offline functionality\n",
    "\n",
    "## Mobile App Testing\n",
    "- [ ] UI responsive on different screen sizes\n",
    "- [ ] Chat history persistence\n",
    "- [ ] Network error handling\n",
    "- [ ] Background/foreground transitions\n",
    "- [ ] Battery usage analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 8: Monitoring and Maintenance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 21: Set Up Logging and Monitoring\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add logging to `main.py`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from google.cloud import logging as cloud_logging\n",
    "\n",
    "# Setup Cloud Logging\n",
    "client = cloud_logging.Client()\n",
    "client.setup_logging()\n",
    "\n",
    "logger = logging.getLogger(\"phi3-chatbot\")\n",
    "\n",
    "@app.post(\"/chat\", response_model=ChatResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    logger.info(f\"Received chat request with conversation_id: {request.conversation_id}\")\n",
    "    \n",
    "    # ... existing code ...\n",
    "    \n",
    "    logger.info(f\"Generated response of length {len(response_text)}\")\n",
    "    \n",
    "    # ... existing code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 22: Create a Monitoring Dashboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Google Cloud Monitoring to create a dashboard:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Install Google Cloud Monitoring agent\n",
    "gcloud beta run services update phi3-api \\\n",
    "    --update-env-vars=\"ENABLE_CLOUD_MONITORING=true\"\n",
    "\n",
    "# Create a custom dashboard (this would typically be done via UI)\n",
    "# But you can use Terraform or gcloud to automate this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Final Steps and Considerations\n",
    "\n",
    "1. **Documentation**: Create comprehensive documentation for your API, model, and mobile app.\n",
    "\n",
    "2. **User Feedback System**: Implement a feedback mechanism to collect user input for model improvements.\n",
    "\n",
    "3. **Update Strategy**: Plan a strategy for model updates and new releases.\n",
    "\n",
    "4. **Privacy Features**: Add encryption for on-device storage and privacy controls.\n",
    "\n",
    "5. **Personalization Systems**: Implement a system to adapt the chatbot's responses based on user interactions.\n",
    "\n",
    "This step-by-step guide covers the entire process of developing an on-device AI chatbot powered by Phi-3 mini, from initial model testing to deployment and maintenance. The implementation aligns with the project requirements of creating a privacy-focused, personalized companion that runs on-device.\n",
    "\n",
    "Would you like me to expand on any particular phase or provide more details about specific technical aspects?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
